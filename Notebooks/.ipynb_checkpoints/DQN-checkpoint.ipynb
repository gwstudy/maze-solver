{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ee757e03-46d7-4f42-b8a9-c7a1fad22d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.current_size = 0\n",
    "        self.queue = deque(maxlen=size)\n",
    "        \n",
    "    def _get_current_size(self):\n",
    "        return self.current_size\n",
    "    \n",
    "    def can_sample(self, size):\n",
    "        return self.current_size < size\n",
    "    \n",
    "    def store(self, transition):\n",
    "        self.current_size += 1\n",
    "        self.queue.enqueue(transition)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        if not self.can_sample(size):\n",
    "            raise Exception('Cannot sample, not enough experience')\n",
    "            \n",
    "        return random.sample(self.queue, size)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        target_net,\n",
    "        update_net,\n",
    "        optimiser, \n",
    "        loss_func,\n",
    "        w_sync_freq=10,\n",
    "        batch_size=10,\n",
    "        memory_size=5000,\n",
    "        gamma=0.95,\n",
    "        step_size=0.001,\n",
    "        episodes=1000,\n",
    "        eval_episodes=50,\n",
    "        epsilon_start=0.3,\n",
    "        epsilon_decay=0.9996,\n",
    "        epsilon_min=0.01,\n",
    "        negative_rewards=[-0.75, -0.85, -15.0],\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = np.float64(gamma)\n",
    "        self.n_states = self.env.observation_space.n\n",
    "        self.states = self.env.states\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.actions = self.env.actions\n",
    "        self.update_net = update_net\n",
    "        self.target_net = target_net\n",
    "        self.memory_size = memory_size\n",
    "        self.replay_memory = ReplayMemory(size=memory_size)\n",
    "        self.optimiser = optimiser\n",
    "        self.loss_func = loss_func\n",
    "        self.step_size = step_size\n",
    "        self.episodes = episodes\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.negative_rewards = negative_rewards\n",
    "        self.eval_episodes = eval_episodes\n",
    "        \n",
    "        # initialize action-value function\n",
    "        self.Q = defaultdict(\n",
    "            lambda: np.zeros(self.n_actions),\n",
    "        )\n",
    "        \n",
    "        # initialize traning logs\n",
    "        self.logs = defaultdict(\n",
    "            lambda: {\n",
    "                'reward': 0,\n",
    "                'cumulative_reward': 0,\n",
    "                'epsilon': None\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        #initialize evaluation logs\n",
    "        self.eval_logs = defaultdict(\n",
    "            lambda: {\n",
    "                'reward': 0,\n",
    "                'cumulative_reward': 0,\n",
    "                'epsilon': None\n",
    "            },\n",
    "        )\n",
    "       \n",
    "    @staticmethod\n",
    "    def _clip_reward(reward):\n",
    "        return (2 * (\n",
    "            reward - self.env.min_reward\n",
    "        ) / (self.env.max_reward - self.env.min_reward)) - 1\n",
    "    \n",
    "    def _get_action_probs(self, state, epsilon):\n",
    "        state = torch.FloatTensor(state)\n",
    "        # initialize episilon probability to all the actions\n",
    "        probs = np.ones(self.n_actions) * (epsilon / self.n_actions)\n",
    "        print(f'state: {state}, type: {type(state)}')\n",
    "        # CHANGE\n",
    "        action_values = self.update_net.forward(state)\n",
    "        best_action = torch.max(action_values, 1)[1].data.numpy()\n",
    "        best_action = best_action[0] if ENV_A_SHAPE == 0 else best_action.reshape(ENV_A_SHAPE)\n",
    "        # initialize 1-epsilon probability to the greedy action\n",
    "        probs[best_action] = 1 - epsilon + (epsilon / self.n_actions)\n",
    "        return probs\n",
    "        \n",
    "    def _get_action(self, state, epsilon):\n",
    "        action = np.random.choice(\n",
    "            self.actions, \n",
    "            p=self._get_action_probs(\n",
    "                state,\n",
    "                epsilon,\n",
    "            ),\n",
    "        ) \n",
    "        \n",
    "        return action, self.actions.index(action)\n",
    "    \n",
    "    def _store_transition(self, transition):\n",
    "        self.replay_memory.store(transition)\n",
    "        \n",
    "    def _train_one_batch(self, transitions, epsilon):\n",
    "        states, actions, rewards, next_states = transitions\n",
    "        \n",
    "        Q_states = self.update_net(states).gather(1, actions)\n",
    "        Q_targets = rewards + self.gamma * self.target_net(next_states).detach().max(1)[0]\n",
    "        \n",
    "        loss = self.loss_func(Q_states, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backwards(retain_variables = True)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def _sync_weights(self):\n",
    "        self.target_net.load_state_dict(self.update_net.state_dict())\n",
    "        \n",
    "    def run(self):\n",
    "        epsilon = self.epsilon_start\n",
    "        for episode_no in range(self.episodes):\n",
    "            print(f'Episode: {episode_no}')\n",
    "            epsilon = max(epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "            episode_ended = False\n",
    "            self.logs[episode_no]['epsilon'] = epsilon\n",
    "            episode_reward = 0\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while not episode_ended:\n",
    "                action, action_idx = self._get_action(state, epsilon)\n",
    "                _, reward, goal, next_state, episode_ended = self.env.step(action=action)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                # ensure gradients are well conditioned \n",
    "                clipped_reward = self._clip_reward(reward)\n",
    "                \n",
    "                self._store_transition(\n",
    "                    [torch.FloatTensor(_) for _ in [state, action_idx, clipped_reward, next_state]]\n",
    "                )\n",
    "                \n",
    "                if self.replay_memory.can_sample(size=self.batch_size):\n",
    "                    transitions = self.replay_memory.sample(size=self.batch_size)\n",
    "                    self._train_one_batch(transitions, epsilon)\n",
    "                \n",
    "            \n",
    "            if episode_no % self.w_sync_freq == 0:\n",
    "                self._sync_weights()\n",
    "            \n",
    "            # save logs for analysis\n",
    "            self.logs[episode_no]['reward'] = episode_reward\n",
    "            if episode_no > 0:\n",
    "                self.logs[episode_no]['cumulative_reward'] += \\\n",
    "                self.logs[episode_no-1]['cumulative_reward']\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
