{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import FloatTensor as FT, tensor as T\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.current_size = 0\n",
    "        self.queue = deque(maxlen=size)\n",
    "        \n",
    "    def _get_current_size(self):\n",
    "        return self.current_size\n",
    "    \n",
    "    def can_sample(self, size):\n",
    "        return self.current_size >= size\n",
    "    \n",
    "    def store(self, transition):\n",
    "        self.current_size += 1\n",
    "        self.queue.append(transition)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        if not self.can_sample(size):\n",
    "            raise Exception('Cannot sample, not enough experience')\n",
    "        \n",
    "        return random.sample(self.queue, size)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        target_net,\n",
    "        policy_net,\n",
    "        optimizer, \n",
    "        loss_func,\n",
    "        model_path,\n",
    "        env_type='image',\n",
    "        log_freq=10,\n",
    "        tau = 1e-3,\n",
    "        train_freq=5,\n",
    "        w_sync_freq=5,\n",
    "        batch_size=10,\n",
    "        memory_size=5000,\n",
    "        gamma=0.95,\n",
    "        step_size=0.001,\n",
    "        episodes=1000,\n",
    "        eval_episodes=50,\n",
    "        epsilon_start=0.3,\n",
    "        epsilon_decay=0.9996,\n",
    "        epsilon_min=0.01,\n",
    "        negative_rewards=[-0.75, -0.85, -15.0],\n",
    "        load_pretrained=False,\n",
    "        save_pretrained=True,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.env_type = env_type\n",
    "        self.gamma = np.float64(gamma)\n",
    "        self.n_states = self.env.observation_space.n\n",
    "        self.states = self.env.states\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.actions = self.env.actions\n",
    "        self.model_path = model_path\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.save_pretrained = save_pretrained\n",
    "        if load_pretrained and os.path.exists(f'{model_path}/policy_net') and os.path.exists(f'{model_path}/target_net'):\n",
    "            print('Pretrained Models loaded')\n",
    "            self.policy_net.load_state_dict(torch.load(f'{model_path}/policy_net'))\n",
    "            self.target_net.load_state_dict(torch.load(f'{model_path}/target_net'))\n",
    "            \n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_memory = ReplayMemory(size=memory_size)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        self.step_size = step_size\n",
    "        self.tau = tau\n",
    "        self.episodes = episodes\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.negative_rewards = negative_rewards\n",
    "        self.eval_episodes = eval_episodes\n",
    "        self.w_sync_freq = w_sync_freq\n",
    "        self.train_freq = train_freq\n",
    "        self.log_freq = log_freq\n",
    "        self.batch_no = 0\n",
    "        self.load_pretrained = load_pretrained\n",
    "        \n",
    "        # initialize action-value function\n",
    "        self.Q = defaultdict(\n",
    "            lambda: np.zeros(self.n_actions),\n",
    "        )\n",
    "        \n",
    "        # initialize traning logs\n",
    "        self.logs = defaultdict(\n",
    "            lambda: {\n",
    "                'reward': 0,\n",
    "                'cumulative_reward': 0,\n",
    "                'epsilon': None\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        #initialize evaluation logs\n",
    "        self.eval_logs = defaultdict(\n",
    "            lambda: {\n",
    "                'reward': 0,\n",
    "                'cumulative_reward': 0,\n",
    "                'epsilon': None\n",
    "            },\n",
    "        )\n",
    "       \n",
    "    def _clip_reward(self, reward):\n",
    "        return (2 * (\n",
    "            reward - self.env.min_reward\n",
    "        ) / (self.env.max_reward - self.env.min_reward)) - 1\n",
    "    \n",
    "    def _get_action_probs(self, state, epsilon):            \n",
    "        # initialize episilon probability to all the actions\n",
    "        probs = np.ones(self.n_actions) * (epsilon / self.n_actions)\n",
    "        action_values = self.policy_net.forward(state.unsqueeze(0))\n",
    "        best_action = torch.argmax(action_values)\n",
    "        # initialize 1-epsilon probability to the greedy action\n",
    "        probs[best_action] = 1 - epsilon + (epsilon / self.n_actions)\n",
    "        return probs\n",
    "        \n",
    "    def _get_action(self, state, epsilon):\n",
    "        if self.env_type == 'image': \n",
    "            oned_state, state = state\n",
    "            \n",
    "        action = np.random.choice(\n",
    "            self.actions, \n",
    "            p=self._get_action_probs(\n",
    "                FT(state),\n",
    "                epsilon,\n",
    "            ),\n",
    "        ) \n",
    "        \n",
    "        return action, self.actions.index(action)\n",
    "    \n",
    "    def _store_transition(self, transition):\n",
    "        self.replay_memory.store(transition)\n",
    "\n",
    "    def _train_one_batch(self, transitions, epsilon):\n",
    "        if self.env_type == 'image':\n",
    "            oned_states, states, actions, rewards, oned_next_states, next_states, goal_achieved = zip(*transitions)\n",
    "        else:\n",
    "            states, actions, rewards, next_states, goal_achieved = zip(*transitions)\n",
    "        \n",
    "        states = FT(states)\n",
    "        next_states = FT(next_states)\n",
    "        \n",
    "        actions = T([actions]).view(-1, 1)\n",
    "        rewards = T([rewards]).view(-1, 1)\n",
    "        goal_achieved = T([goal_achieved]).view(-1, 1).float()\n",
    "        \n",
    "        Q_values = self.policy_net(states)\n",
    "        \n",
    "        predictions = Q_values.gather(1, actions)\n",
    "        labels_next = torch.max(self.target_net(next_states), dim=1).values.view(-1, 1).detach()            \n",
    "        labels = rewards + (self.gamma * labels_next * (1 - goal_achieved))\n",
    "        \n",
    "        loss = self.loss_func(predictions, labels)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def _sync_weights(self, soft=False):\n",
    "        if not soft:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        else:\n",
    "            # @TODO: Implement soft updates\n",
    "            pass\n",
    "        \n",
    "    def run(self):\n",
    "        epsilon = self.epsilon_start\n",
    "        for episode_no in range(self.episodes):\n",
    "            epsilon = max(epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "            episode_ended = False\n",
    "            self.logs[episode_no]['epsilon'] = epsilon\n",
    "            episode_reward = 0\n",
    "            episode_loss = 0\n",
    "            timestep = 0\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while not episode_ended:\n",
    "                action, action_idx = self._get_action(state, epsilon)\n",
    "                _, reward, done, next_state, episode_ended = self.env.step(action=action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if self.env_type == 'image':\n",
    "                    self._store_transition(\n",
    "                        [state[0], state[1], action_idx, reward, next_state[0], next_state[1], done]\n",
    "                    )\n",
    "                else:\n",
    "                    self._store_transition(\n",
    "                        [state, action_idx, reward, next_state, done]\n",
    "                    )\n",
    "                \n",
    "                if self.replay_memory.current_size > self.memory_size:\n",
    "                    transitions = self.replay_memory.sample(size=self.batch_size)\n",
    "                    if timestep % self.train_freq == 0:\n",
    "                        episode_loss += self._train_one_batch(transitions, epsilon)\n",
    "                    \n",
    "                    if self.batch_no % self.w_sync_freq == 0:\n",
    "                        self._sync_weights()\n",
    "                    self.batch_no += 1\n",
    "                    \n",
    "                timestep += 1\n",
    "                state = next_state\n",
    "                \n",
    "            if episode_no % self.log_freq == 0:\n",
    "                print(f'Episode: {episode_no}, Reward: {episode_reward}, Loss: {episode_loss}')\n",
    "                \n",
    "            # save logs for analysis\n",
    "            self.logs[episode_no]['reward'] = episode_reward\n",
    "            if episode_no > 0:\n",
    "                self.logs[episode_no]['cumulative_reward'] += \\\n",
    "                self.logs[episode_no-1]['cumulative_reward']\n",
    "                \n",
    "        self.save_models()\n",
    "                \n",
    "    def evaluate_one_episode(self, e_num=None, policy=None):\n",
    "        action_seq = []\n",
    "        timestep = 0\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "            \n",
    "        while not done:\n",
    "            action, reward, goal, state, done = self.env.step(\n",
    "                action=self._get_action(state, 0)[0],\n",
    "            )\n",
    "            timestep += 1\n",
    "            \n",
    "            if e_num is not None:\n",
    "                self.eval_logs[e_num]['reward'] += reward\n",
    "                self.eval_logs[e_num]['cumulative_reward'] = self.eval_logs[e_num]['reward']\n",
    "                self.eval_logs[e_num]['goal_achieved'] = goal\n",
    "            \n",
    "            action_seq.append(action)\n",
    "            \n",
    "        return timestep, action_seq\n",
    "    \n",
    "    def evaluate(self, policy=None):\n",
    "        for n in range(self.eval_episodes):\n",
    "            timesteps, _ = self.evaluate_one_episode(n, policy)\n",
    "            self.eval_logs[n]['timesteps'] = timesteps\n",
    "            \n",
    "            if n > 0:\n",
    "                self.eval_logs[n]['cumulative_reward'] += \\\n",
    "                self.eval_logs[n-1]['cumulative_reward']\n",
    "                \n",
    "    def save_models(self):\n",
    "        torch.save(self.target_net.state_dict(), f'{self.model_path}/target_net')\n",
    "        torch.save(self.policy_net.state_dict(), f'{self.model_path}/policy_net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
